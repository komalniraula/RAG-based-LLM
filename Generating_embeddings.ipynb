{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9akXsc0JISj",
        "outputId": "6781ea27-a81a-46fc-8f47-3824854a8713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
            "  Downloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.11 langchain-text-splitters-0.2.2 langsmith-0.1.83 orjson-3.10.6\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRpxJb7Oro7B"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03f3WdJorhHJ"
      },
      "outputs": [],
      "source": [
        "# Read pdfs from uploaded directory\n",
        "def read_pdfs(directory):\n",
        "    texts = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            reader = PdfReader(file_path)\n",
        "            raw_text = \"\"\n",
        "            for page in reader.pages:\n",
        "                raw_text += page.extract_text()\n",
        "            texts[filename] = raw_text\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
        "    start = 0\n",
        "    chunks = []\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - chunk_overlap\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "TecfmKTRR5N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbxwaaGSsvef"
      },
      "outputs": [],
      "source": [
        "pdf_directory = \"Documents\"\n",
        "pdf_texts = read_pdfs(pdf_directory)\n",
        "pdf_chunks = {filename: split_text(text) for filename, text in pdf_texts.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CWwO2FucdNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8acd1051-9419-4545-83ae-733865c32e40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NewModel(\n",
              "  (embeddings): NewEmbeddings(\n",
              "    (word_embeddings): Embedding(30528, 1024, padding_idx=0)\n",
              "    (rotary_emb): NTKScalingRotaryEmbedding()\n",
              "    (token_type_embeddings): Embedding(2, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): NewEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x NewLayer(\n",
              "        (attention): NewAttention(\n",
              "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        )\n",
              "        (mlp): NewGatedMLP(\n",
              "          (up_gate_proj): Linear(in_features=1024, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act_fn): GELUActivation()\n",
              "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (attn_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "        (mlp_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "        (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "#model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "#model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gpDcvsfIsio"
      },
      "outputs": [],
      "source": [
        "# Function to embed text\n",
        "def embed_text(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5wmtUzTIsip",
        "outputId": "b70788c6-43ec-404a-8b58-9507c95101e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding chunks from Operations Manual- 2080.pdf:  20%|█▉        | 65/330 [03:13<12:37,  2.86s/it]"
          ]
        }
      ],
      "source": [
        "embeddings = {}\n",
        "for filename, chunks in pdf_chunks.items():\n",
        "    chunk_embeddings = []\n",
        "    for chunk in tqdm(chunks, desc=f\"Embedding chunks from {filename}\", leave=False):\n",
        "        chunk_embeddings.append(embed_text(chunk, tokenizer, model))\n",
        "    embeddings[filename] = np.array(chunk_embeddings, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings and splits to disk\n",
        "for filename, emb in embeddings.items():\n",
        "    np.save(f'{filename}_embeddings.npy', emb)\n",
        "    with open(f'{filename}_chunks.npy', 'wb') as f:\n",
        "        np.save(f, pdf_chunks[filename])"
      ],
      "metadata": {
        "id": "9AjFUxhfSNob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all embeddings into a single matrix for indexing\n",
        "all_embeddings = np.concatenate([emb for emb in embeddings.values()], axis=0)\n",
        "dimension = all_embeddings.shape[1]"
      ],
      "metadata": {
        "id": "R5Gxn4MTSPG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and populate the Faiss index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(all_embeddings)\n",
        "faiss.write_index(index, 'faiss_index.index')"
      ],
      "metadata": {
        "id": "ewnNVzhWSUv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metadata to map embeddings to original chunks\n",
        "metadata = []\n",
        "offset = 0\n",
        "for filename, emb in embeddings.items():\n",
        "    for i in range(len(emb)):\n",
        "        metadata.append((filename, i + offset))\n",
        "    offset += len(emb)"
      ],
      "metadata": {
        "id": "Ocl0A3t5SYZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('metadata.npy', metadata)"
      ],
      "metadata": {
        "id": "suIjhlUDSa60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChcB6yPiIsir"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}